use std::{
	convert::{TryFrom, TryInto},
	hash::Hasher,
	io::{self, SeekFrom},
	lazy::SyncLazy,
	num::TryFromIntError,
	path::Path,
	string::FromUtf8Error,
	time::SystemTime
};

use aes::Aes128;
use ahash::AHashMap;
use futures::{StreamExt, TryStreamExt};
use static_assertions::const_assert;
use thiserror::Error;
use tokio::{
	fs::File,
	io::{AsyncRead, AsyncReadExt, AsyncSeek, AsyncSeekExt, AsyncWriteExt},
	task
};
use tokio_stream::Stream;
use tokio_util::io::ReaderStream;
use zopfli::Format;

use self::{
	buffered_async_spooled_temp_file::BufferedAsyncSpooledTempFile,
	double_offset_stream_decorator::DoubleOffsetStreamDecorator,
	file_region_comparator::FileRegionComparator,
	relative_path::RelativePath,
	system_time_sanitizer::{SystemTimeSanitizationError, SystemTimeSanitizer},
	zip_file_record::{
		CentralDirectoryHeader, CompressionMethod, EndOfCentralDirectory, LocalFileHeader
	}
};

mod buffered_async_spooled_temp_file;
mod double_offset_stream_decorator;
mod file_region_comparator;
mod relative_path;
mod system_id;
mod system_time_sanitizer;
mod zip_file_record;

#[cfg(test)]
mod tests;

// We assume usize is at least 16 bits wide to do proper conversions
const_assert!(usize::BITS >= 16);

/// Contains information about a file that was processed in a previous
/// run of PackSquash; i.e., already present in a generated ZIP file.
struct PreviousFile {
	/// Time when this file was processed in the previous run.
	squash_time: SystemTime,
	/// The offset to (seek position in the file of) the processed,
	// compressed data.
	data_offset: u64,
	/// The CRC of the processed data in the previous ZIP file. This field
	/// will be passed through.
	crc32: u32,
	/// The compression method used in the previous ZIP file. This field
	/// will be passed through.
	compression_method: CompressionMethod,
	/// The size of the uncompressed version of the previous ZIP file data.
	/// This field will be passed through.
	uncompressed_size: u32,
	/// The size of the compressed version of the previous ZIP file data.
	/// This field will be passed through and used to copy the file data.
	compressed_size: u32
}

/// A partial central directory header record, which stores the minimal data
/// needed to generate the actual central directory header at some point.
struct PartialCentralDirectoryHeader {
	local_header_offset: u64,
	file_name: String,
	compression_method: CompressionMethod,
	squash_time: [u8; 4],
	crc32: u32,
	compressed_size: u32,
	uncompressed_size: u32
}

/// Represents a ZIP file hash and size pair.
#[derive(PartialEq, Eq, Hash)]
struct HashAndSize {
	hash: u32,
	size: u32
}

/// Represents an error that may happen during a fallible SquashZip operation.
#[derive(Error, Debug)]
#[non_exhaustive]
enum SquashZipError {
	#[error("Invalid previous ZIP: {0}. Was it generated by PackSquash?")]
	InvalidPreviousZip(&'static str),
	#[error("A filename in the previous ZIP file is not valid UTF-8: {0}")]
	InvalidFileName(#[from] FromUtf8Error),
	#[error("Unknown compression method in previous ZIP: {0}. Was it generated by PackSquash?")]
	UnknownCompressionMethod(u16),
	#[error(
		"Tried to handle a value that is off limits: {0}. Is some file, file name or count so big?"
	)]
	Overflow(#[from] TryFromIntError),
	#[error("A file size exceeds the 4 GiB limit")]
	FileTooBig,
	#[error("Could not create a timestamp for a ZIP file: {0}")]
	SystemTimeSanitizationError(#[from] SystemTimeSanitizationError),
	#[error("No such file in the previous ZIP: {0}")]
	NoSuchPreviousFile(String),
	#[error("I/O error: {0}")]
	Io(#[from] io::Error)
}

/// Contains settings that change SquashZip operation.
pub(super) struct SquashZipSettings {
	/// This controls whether SquashZip will try to conform to the ZIP
	/// specification strictly. In case of ambiguity or incomplete specification,
	/// it will do whatever other programs do, whatever provides more options for
	/// data error detection and recovery, whatever allows for simpler or more diverse
	/// reader implementations, or whatever seems closer to the implicit intent of
	/// the specification, with the hopes of maximizing interoperability.
	///
	/// If strict compliance is disabled, PackSquash will disregard any of the
	/// previously mentioned concerns, favoring the smallest possible ZIP file size
	/// that can only be read by Minecraft, without any guarantee that it will work with
	/// other ZIP file software. A Java 8 ZipFile implementation is assumed, whose
	/// source code can be seen [here][1] and [here][2]. For Java 11, JNI parts were
	/// reimplemented in Java and several minor fixes were rolled out, as seen [here][3].
	///
	/// [1]: https://code.yawk.at/java/8/java/util/zip/ZipFile.java
	/// [2]: https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/master/jdk/src/share/native/java/util/zip/zip_util.c
	/// [3]: https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/master/src/java.base/share/classes/java/util/zip/ZipFile.java
	strict_spec_conformance: bool,
	/// SquashZip uses a spool buffer instead of a proper output file to speed up seeks
	/// and writes as much as possible. However, memory is not indefinitely big, and if
	/// the result ZIP file grows bigger than the available memory, PackSquash will run
	/// out of memory. This setting sets the cut-off size point when PackSquash will
	/// roll out the content of the spool buffer to disk and carry on performing further
	/// operations there, to allow for increased stability at the cost of performance.
	spool_buffer_size: usize
}

/// A custom, minimalistic ZIP compressor, which exploits its great control
/// over the low-level details of the ZIP format to make some PackSquash
/// optimizations and use cases possible.
pub(super) struct SquashZip<F: AsyncRead + AsyncSeek + Unpin> {
	settings: SquashZipSettings,
	output_zip: BufferedAsyncSpooledTempFile,
	output_zip_size: u64,
	previous_zip: Option<F>,
	previous_zip_contents: AHashMap<RelativePath<'static>, PreviousFile>,
	processed_local_headers: AHashMap<HashAndSize, Vec<(u64, u32)>>,
	central_directory_data: Vec<PartialCentralDirectoryHeader>,
	crc32_hasher: crc32fast::Hasher
}

/// The system time sanitizer that SquashZip will use for sanitizing and
/// desanitizing dates to and from ZIP files, respectively.
static SYSTEM_TIME_SANITIZER: SyncLazy<SystemTimeSanitizer<Aes128>> =
	SyncLazy::new(SystemTimeSanitizer::new);

impl<F: AsyncRead + AsyncSeek + Unpin> SquashZip<F> {
	/// Creates a new instance of this struct, thay may leverage the
	/// results of a ZIP file generated in a previous run to speed up
	/// the process of compressing the current resource pack.
	///
	/// Any previous ZIP file passed to this method is assumed to have
	/// been generated and/or modified only by SquashZip. This method
	/// does some sanity checks to verify that assumption, but they are
	/// not completely reliable by design.
	async fn new(
		mut previous_zip: Option<F>,
		settings: SquashZipSettings
	) -> Result<Self, SquashZipError> {
		let mut previous_zip_contents;

		if let Some(previous_zip) = &mut previous_zip {
			let mut buffer = [0u8; 52];

			// ZIP files generated by SquashZip have no comments, and always have
			// their mandatory end of central directory record at the very end. We can't
			// support ZIP files generated by other programs easily and reliably, so this
			// simplification also serves as a weak sanity check
			previous_zip.seek(SeekFrom::End(-22)).await?;
			previous_zip.read_exact(&mut buffer[..4]).await?;
			if buffer[..4] != [0x50, 0x4B, 0x05, 0x06] {
				return Err(SquashZipError::InvalidPreviousZip(
					"EOCD signature not found at the expected position"
				));
			}

			// Now read fields that are relevant to populate our previous ZIP contents map

			// We are just after the end of central directory signature. Read several
			// fields of that record at once for speed: number of this disk, number of disk
			// with start of CD, number of CD entries in this disk, number of total CD entries,
			// CD size, and offset to CD (2 + 2 + 2 + 2 + 4 + 4 = 16 bytes)
			previous_zip.read_exact(&mut buffer[..16]).await?;

			// This entry count may be incorrect if we either are using ZIP64 extensions
			// or a proper count was not written. We may get a better hint if we check the ZIP64
			// extensions, but doing so opens the possibility of exhausting all the available memory
			// with specially crafted ZIP files, so we don't do that and instead reallocate later if
			// really needed
			let cdh_entry_count_hint = u16::from_le_bytes(buffer[6..8].try_into().unwrap()) as usize;
			let mut central_directory_offset =
				u32::from_le_bytes(buffer[12..16].try_into().unwrap()) as u64;

			if central_directory_offset == 0xFFFFFFFF {
				// We maybe have a proper offset in a ZIP64 end of central directory
				// record. Use its locator to find it

				// Move to the beginning of the locator. It's just before the end of CD
				previous_zip.seek(SeekFrom::Current(-40)).await?;

				// Read signature, disk number and offset (4 + 4 + 8 = 16 bytes)
				previous_zip.read_exact(&mut buffer[..16]).await?;

				// Check locator signature
				if buffer[..4] == [0x50, 0x4B, 0x06, 0x07] {
					// Get where the ZIP64 end of central directory record is
					let zip64_end_of_central_directory_offset =
						u64::from_le_bytes(buffer[8..16].try_into().unwrap());

					// Check its signature
					previous_zip
						.seek(SeekFrom::Start(zip64_end_of_central_directory_offset))
						.await?;
					previous_zip.read_exact(&mut buffer[..52]).await?;
					if buffer[..4] != [0x50, 0x4B, 0x06, 0x06] {
						return Err(SquashZipError::InvalidPreviousZip(
							"EOCD64 signature expected, but not found"
						));
					}

					// Hooray, we found the proper central directory offset!
					central_directory_offset = u64::from_le_bytes(buffer[44..52].try_into().unwrap());
				} else {
					// This is not an error. The offset may indeed be all-ones, although
					// this is very rare. Continue anyway; if the file is corrupt, we will
					// likely error out later
				}
			}

			// Seek to the offset of the first central directory header
			previous_zip
				.seek(SeekFrom::Start(central_directory_offset))
				.await?;

			// Create a map with the most appropriate capacity, given the limitations
			// of the entry count hint
			previous_zip_contents = AHashMap::with_capacity(cdh_entry_count_hint);

			// Keep adding files to the map until there are no more central directory headers
			while {
				previous_zip.read_exact(&mut buffer[..4]).await?;
				buffer[..4] == [0x50, 0x4B, 0x01, 0x02]
			} {
				// Read all the remaining central directory header fields
				previous_zip.read_exact(&mut buffer[..42]).await?;

				let file_comment_length = u16::from_le_bytes(buffer[28..30].try_into().unwrap());
				let extra_field_length = u16::from_le_bytes(buffer[26..28].try_into().unwrap());

				// SquashZip never generates comments in ZIP files
				if file_comment_length > 0 {
					return Err(SquashZipError::InvalidPreviousZip(
						"File comment found, but not expected"
					));
				}

				// SquashZip either generates no extra fields or a single ZIP64 data field
				// with a extended local header offset (2 + 2 + 8 = 12 bytes)
				if extra_field_length != 0 && extra_field_length != 12 {
					return Err(SquashZipError::InvalidPreviousZip(
						"Unexpected extra fields size in CDH"
					));
				}

				// Read the fields that will be stored as-is in the map
				let process_time = SYSTEM_TIME_SANITIZER
					.desanitize(buffer[8..12].try_into().unwrap(), &buffer[12..16]);
				// TODO: decrypt this with FPE if needed. Handle obfuscation in this method
				let crc = u32::from_le_bytes(buffer[12..16].try_into().unwrap());
				let compression_method = CompressionMethod::from_compression_method_field(
					u16::from_le_bytes(buffer[6..8].try_into().unwrap())
				)?;
				let compressed_size = u32::from_le_bytes(buffer[16..20].try_into().unwrap());
				let uncompressed_size = u32::from_le_bytes(buffer[20..24].try_into().unwrap());

				// Read the fields that we will use for further parsing
				let file_name_length =
					u16::from_le_bytes(buffer[24..26].try_into().unwrap()) as usize;
				let mut local_file_header_offset =
					u32::from_le_bytes(buffer[38..42].try_into().unwrap()) as u64;

				// Now get the relative path
				let relative_path;
				{
					// The filename may not only be larger than our stack-allocated buffer,
					// but we also need a owned string because that buffer is dropped when
					// this function ends
					let mut filename_buf = vec![0; file_name_length];

					previous_zip.read_exact(&mut filename_buf).await?;

					// In the unlikely case this relative path is corrupt and/or invalid, but
					// still valid UTF-8, it'll be effectively ignored, so it doesn't really
					// matter
					relative_path = RelativePath::new_from_string(String::from_utf8(filename_buf)?);
				}

				if extra_field_length == 12 && local_file_header_offset == 0xFFFFFFFF {
					// We maybe have a proper local file header offset in a ZIP64 extended information
					// extra field. Read it. It's just after the file name
					previous_zip.read_exact(&mut buffer[..12]).await?;

					// Check the ZIP64 field tag to make sure it really is the field we're looking for
					if buffer[..2] == [0x01, 0x00] {
						local_file_header_offset =
							u64::from_le_bytes(buffer[4..12].try_into().unwrap());
					} else {
						// This wouldn't be a format error, as the extra fields are just a list of blocks.
						// However, SquashZip doesn't generate ZIP files with extra fields other than this,
						// so this definitely means that the ZIP file was modified or corrupted
						return Err(SquashZipError::InvalidPreviousZip(
							"Found extra field in CDH that is not a ZIP64 extended information field"
						));
					}
				}

				// Assume that current offset is where the next central directory header starts.
				// This is true because we have read the extra fields, if any, and there are no
				// comments. If there were extra fields, but we didn't read them, we'll error out
				// when looking for the next central directory header, because the seek position
				// will point to those fields. This is intentional, as that signals a non-SquashZip
				// ZIP file, and we should error out with such a file
				let next_central_directory_header_offset =
					previous_zip.seek(SeekFrom::Current(0)).await?;

				// Now go to the local file header. We need to parse it a bit to compute the
				// compressed data offset, as the compressed data is after the file name, which
				// has variable length. We can't assume that the local hile header filename
				// length is the same as in the central directory because we intentionally
				// set it to zero to save space when not being strictly compliant
				previous_zip
					.seek(SeekFrom::Start(local_file_header_offset))
					.await?;

				// Read all the local file header fields, barring file name and extra fields
				previous_zip.read_exact(&mut buffer[..30]).await?;

				// Check signature
				if buffer[..4] != [0x50, 0x4B, 0x03, 0x04] {
					return Err(SquashZipError::InvalidPreviousZip(
						"LFH signature not found at expected position"
					));
				}

				let local_header_file_name_length =
					u16::from_le_bytes(buffer[26..28].try_into().unwrap()) as usize;
				let extra_field_length = u16::from_le_bytes(buffer[28..30].try_into().unwrap());

				if local_header_file_name_length != 0
					&& local_header_file_name_length != file_name_length
				{
					// Again, according to the specification, this shouldn't be a hard error,
					// but for our purposes it indicates that this ZIP file is not pristine
					return Err(SquashZipError::InvalidPreviousZip(
						"Unexpected file name length in LFH"
					));
				}

				if extra_field_length > 0 {
					// SquashZip ZIP files never contain extra fields in local file headers
					return Err(SquashZipError::InvalidPreviousZip(
						"Unexpected extra field length in LFH"
					));
				}

				// After all this work, we can finally insert the file data in the map :)
				previous_zip_contents.insert(
					relative_path,
					PreviousFile {
						squash_time: process_time,
						data_offset: local_file_header_offset
							+ 30 + local_header_file_name_length as u64,
						crc32: crc,
						compression_method,
						uncompressed_size,
						compressed_size
					}
				);

				// Make sure the seek position points to the next central directory header for the
				// next iteration
				previous_zip
					.seek(SeekFrom::Start(next_central_directory_header_offset))
					.await?;
			}
		} else {
			// No previous contents if no previous file to read their data from
			previous_zip_contents = AHashMap::new();
		}

		// TODO: initial obfuscation in output ZIP

		Ok(Self {
			output_zip: BufferedAsyncSpooledTempFile::new(settings.spool_buffer_size),
			output_zip_size: 0,
			settings,
			previous_zip,
			processed_local_headers: AHashMap::with_capacity(previous_zip_contents.len()),
			central_directory_data: Vec::with_capacity(previous_zip_contents.len()),
			previous_zip_contents,
			crc32_hasher: crc32fast::Hasher::new()
		})
	}

	/// Adds a new file to the result ZIP file from its path and a stream of its
	/// processed contents.
	///
	/// Callers should take into account whether a suitable previous version of
	/// the file, in order to add it more cheaply by calling [`Self::add_previous_file()`].
	/// In that case, it is an error to call both methods for the same file: behavior is
	/// **undefined**.
	///
	/// Adding several files with the same path will not cause this function to fail,
	/// but doing so will generate ZIP files that make little sense on a semantic level
	/// for no good reasons. Therefore, doing so is not recommended.
	///
	/// The result ZIP file may be left in an inconsistent state if this method returns
	/// an error. The caller probably should discard the ZIP file if this happens, not
	/// not calling any further methods on this instance.
	async fn add_file<'a, S: Stream<Item = &'a [u8]> + Unpin>(
		&mut self,
		path: RelativePath<'_>,
		processed_data: &mut S,
		skip_compression: bool
	) -> Result<(), SquashZipError> {
		// This big method implements the algortihm described in the following steps:
		// 0. Store the current file offset and Squash Time timestamp.
		// 1. Create a local file header.
		// 2. Write as many zero bytes as the local file header will take (i.e. reserve
		//    space for it in the output file).
		// 3. Write processed, but uncompressed data from the processed_data stream to
		//    the output file. Store its size and CRC32 hash. Update the local file
		//    header with the CRC32, uncompressed data size, and Squash Time.
		// 4. Compress the processed data right after the end of the data written in 3.
		//    Store the "compressed, processed data" size. Store the file offsets where
		//    both the "compressed, processed data" and "processed data" start.
		// 5. Compare len(compressed, processed data) with len(processed data).
		//    5.1. If len(compressed, processed data) < len(processed data), move
		//         "compressed, processed data" to where "processed data" is. Store that
		//         DEFLATE is the compression method used.
		//    5.2. If len(compressed, processed data) >= len(processed data), store that
		//         STORE is the compression method used and make "compressed, processed
		//         data" equal to the "processed data" size computed in 3.
		// 6. Get entry for map 1) (CRC32 hash, processed data size) -> (local file header
		//    offset list). This map stores offsets that point to already written local
		//    file headers of files with some CRC32 and size combination, so if strict ZIP
		//    spec conformance is not desired this allows to check whether a file with the
		//    same data was already added to the ZIP file.
		//    6.1. If it is in map 1), compare the file data we have just added with the file
		//         datas we have already stored and are pointed to by the offset list.
		//         6.1.1. If some file data matches (bitwise comparison succeeds), add an
		//                entry (partial CEN data list, with matched file local header offset)
		//                to a list 2) and discard the local file header and data added in
		//                previous steps, by rewinding the output file to the position stored
		//                in 0. Do not increment output ZIP file size.
		//         6.1.2. If no file data matches (bitwise comparison fails), push the new
		//                local file header to the value of the entry described in 6, and
		//                add an entry to the list 2) with the new local file header offset.
		//                Increment output ZIP file size by local file header size +
		//                "compressed, processed data" size. Seek the output file to the
		//                position stored in 0 and overwrite the local file header with its
		//                actual, final data. Finally, seek the output file to the position
		//                where the "compressed, processed data" ends.
		//    6.2. If it is not in map 1), proceed as in 6.1.2.
		// Alternate flows:
		// - If strict ZIP conformance is desired, make 6.1 behave like 6.2. Also, do not
		//   write Squash Time during 3.
		// - If no compression is desired, skip 4, treating the processed and uncompressed
		//   data as if it was compressed.

		// Step 0
		let local_file_header_offset = self.output_zip.seek(SeekFrom::Current(0)).await?;

		// Step 1
		let mut local_file_header = LocalFileHeader::new(path.as_ref())?; // TODO: obfuscate path

		// Step 2
		let local_file_header_size = local_file_header.get_size();
		local_file_header
			.reserve_space(&mut self.output_zip)
			.await?;

		// Step 3
		let mut processed_data_size = 0u32;
		let processed_data_hash;

		while let Some(data) = processed_data.next().await {
			self.output_zip.write_all(data).await?;

			self.crc32_hasher.write(data);
			processed_data_size = processed_data_size
				.checked_add(data.len().try_into()?)
				.ok_or(SquashZipError::FileTooBig)?;
		}

		processed_data_hash = self.crc32_hasher.finish() as u32;
		self.crc32_hasher.reset();

		local_file_header.crc32 = processed_data_hash; // TODO: obfuscate
		local_file_header.uncompressed_size = processed_data_size; // TODO: obfuscate
		if !self.settings.strict_spec_conformance {
			local_file_header.squash_time = SYSTEM_TIME_SANITIZER
				.sanitize(&SystemTime::now(), &processed_data_hash.to_le_bytes())?; // TODO: obfuscate
		}

		// Step 4
		let processed_data_start_offset = local_file_header_offset + local_file_header_size as u64;
		let compressed_data_start_offset;
		let mut compressed_data_size;
		if skip_compression {
			compressed_data_start_offset = processed_data_start_offset;
			compressed_data_size = processed_data_size as u64;
		} else {
			compressed_data_start_offset = processed_data_start_offset + processed_data_size as u64;

			self.output_zip
				.seek(SeekFrom::Start(processed_data_start_offset))
				.await?;

			let do_compression = |output_zip: &mut BufferedAsyncSpooledTempFile| -> io::Result<u64> {
				let output_zip_decorator = DoubleOffsetStreamDecorator::new(
					output_zip,
					processed_data_start_offset,
					compressed_data_start_offset
				);

				zopfli::compress(
					&zopfli::Options::default(),
					&Format::Deflate,
					&output_zip_decorator,
					processed_data_size as u64,
					&output_zip_decorator
				)
				.map(|_| output_zip_decorator.get_write_position())
			};

			// Our output file implements non async I/O operations in a way that blocks the current
			// thread in place when it is rolled, and does not block when it is not rolled. But doing
			// Zopfli compression is time consuming, and we want to treat it as a blocking operation,
			// so let I/O do the blocking for us if the file is rolled, and do it once ourselves when
			// not rolled.
			// FIXME: reconsider this, the existence of the is_rolled method and DoubleOffsetStreamDecorator
			// if the output ZIP is ever changed to be really async
			let compressed_data_end_offset;
			if self.output_zip.is_rolled() {
				compressed_data_end_offset = do_compression(&mut self.output_zip)?;
			} else {
				compressed_data_end_offset =
					task::block_in_place(|| do_compression(&mut self.output_zip))?;
			}

			// If for some reason our file contains 4 GiB of uncompressible data, compression
			// overhead can increase its size over 4 GiB. However, this doesn't matter, because we
			// always choose the smaller representation, and a number greater than 4 Gi won't ever
			// be equal than or smaller than a number that is <= 4 Gi
			compressed_data_size = compressed_data_end_offset - compressed_data_start_offset;
		}

		// Step 5
		let compression_method;
		if compressed_data_size < processed_data_size as u64 {
			// Step 5.1: compressed data is smaller, and should replace uncompressed
			// (processed) data in the file
			self.output_zip
				.seek(SeekFrom::Start(compressed_data_start_offset))
				.await?;

			let do_copy = |output_zip: &mut BufferedAsyncSpooledTempFile| -> io::Result<()> {
				let output_zip_decorator = DoubleOffsetStreamDecorator::new(
					output_zip,
					compressed_data_start_offset,
					processed_data_start_offset
				);

				io::copy(
					&mut io::Read::take(&mut &output_zip_decorator, compressed_data_size as u64),
					&mut &output_zip_decorator
				)
				.map(|_| ())
			};

			// As per the reasons mentioned in Step 4 implementation, we handle blocking I/O
			// properly
			if self.output_zip.is_rolled() {
				task::block_in_place(|| do_copy(&mut self.output_zip))?;
			} else {
				do_copy(&mut self.output_zip)?;
			}

			compression_method = CompressionMethod::Deflate;
		} else {
			// Step 5.2: compressed data is equal in size or bigger. Compressed
			// data should be discarded
			compression_method = CompressionMethod::Store;

			compressed_data_size = processed_data_size as u64;
		}

		local_file_header.compressed_size = compressed_data_size as u32; // TODO: obfuscate
		local_file_header.compression_method = compression_method; // TODO: obfuscate

		// Step 6: check if we have already written another local file header that
		// contains this file data
		let mut empty_vec;
		let matching_local_headers = if self.settings.strict_spec_conformance {
			// We can't reuse local file headers if strict conformance is desired.
			// Consider that no headers ever match
			empty_vec = Vec::new();
			&mut empty_vec
		} else {
			self.processed_local_headers
				.entry(HashAndSize {
					hash: processed_data_hash,
					size: compressed_data_size as u32
				})
				.or_insert_with(|| Vec::with_capacity(1)) // Usually, this list will be small
		};

		let get_partial_central_directory_header =
			|local_header_offset: u64| -> PartialCentralDirectoryHeader {
				PartialCentralDirectoryHeader {
					local_header_offset,
					file_name: String::from(path.as_ref()),
					compression_method,
					squash_time: local_file_header.squash_time,
					crc32: processed_data_hash,
					compressed_size: compressed_data_size as u32,
					uncompressed_size: processed_data_size
				}
			};

		let mut already_stored = false;
		for (matching_header_offset, matching_header_size) in &*matching_local_headers {
			let matching_data_start_offset = matching_header_offset + *matching_header_size as u64;

			// Rule out hash collisions by doing an expensive bitwise comparison.
			// Hash collisions for files that are not specially crafted to provoke them should
			// be so rare, but to output correct results no matter what, we need to do the
			// check
			already_stored = FileRegionComparator::new(&mut self.output_zip).eq(
				processed_data_start_offset,
				matching_data_start_offset,
				compressed_data_size as u32
			)?;

			if already_stored {
				// Step 6.1.1: discard the local file header and file data we have just wrote,
				// because an equivalent pair has been written before, and we will reuse
				// those for maximum efficiency. To do this:
				// - We remember to create another central directory header entry that points
				//   to the matching local file header
				// - We rewind to the offset we saved at step 0 and not increment the output
				//   ZIP file size counter
				self.central_directory_data
					.push(get_partial_central_directory_header(
						*matching_header_offset
					));

				self.output_zip
					.seek(SeekFrom::Start(local_file_header_offset))
					.await?;

				// Found a match. No point in finding more
				break;
			}
		}

		if !already_stored {
			// Steps 6.1.2 and 6.2: this file has not been stored yet. We have to:
			// - Add it to the processed_local_headers map, so for future files we know that
			//   a file with this hash and size was stored at the current local file header
			//   offset
			// - Add it to the central_directory_data list, so we create a central directory
			//   file header record that points to the new local file header when we finish
			//   the file
			// - Increment the size of the output ZIP file by the size of the local header,
			//   plus the size of the processed and compressed file data that follows
			// - Seek to the position stored in 0 to overwrite the local file header with its
			//   actual data (in step 2 it was just reserved by writing placeholder bytes)
			// - Seek to the end of the processed and compressed file data, so the next
			//   local file header we add is written just after this file data
			if !self.settings.strict_spec_conformance {
				// Avoid allocating memory for the dummy vector
				matching_local_headers.push((local_file_header_offset, local_file_header_size));
			}

			self.central_directory_data
				.push(get_partial_central_directory_header(
					local_file_header_offset
				));

			self.output_zip_size += local_file_header_size as u64 + compressed_data_size;

			self.output_zip
				.seek(SeekFrom::Start(local_file_header_offset))
				.await?;

			local_file_header.write(&mut self.output_zip).await?;

			self.output_zip
				.seek(SeekFrom::Start(
					processed_data_start_offset + compressed_data_size
				))
				.await?;
		}

		Ok(())
	}

	/// Returns the time the specified file was added to the ZIP file generated by
	/// SquashZip in a previous run. `None` may be returned if, for instance, the
	/// file didn't exist before, or there is no available data about when this file
	/// was added.
	fn get_file_process_time(&self, file_path: &RelativePath<'_>) -> Option<SystemTime> {
		self.previous_zip_contents
			.get(file_path)
			.map(|previous_file| previous_file.squash_time)
	}

	/// Cheaply adds the specified previous run file to the ZIP file that is being generated
	/// right now. By default, all previous run files are not added again to the output ZIP
	/// file.
	///
	/// It is an error to call both [`Self::add_file()`] and [`Self::add_previous_file()`].
	/// As with [`Self::add_file()`], if this method returns an error, this SquashZip instance
	/// will become poisoned: it's no longer guaranteed that the output ZIP file will be correct.
	///
	/// A [`SquashZipError::NoSuchPreviousFile`] error is returned if the specified file path
	/// was not present in the previous ZIP file. In this case it is guaranteed that no bad
	/// state was introduced in the result output ZIP file, and the instance can still used
	/// normally.
	async fn add_previous_file(
		&mut self,
		file_path: &RelativePath<'_>
	) -> Result<(), SquashZipError> {
		// For this method we implement a simpler version of the algorithm explained in
		// add_file. It can be summarised as follows:
		// 1. Check if the file is in map 1) (hash, tamaÃ±o) -> (LOC offset list).
		//    1.1. It is (there is an entry and a comparison is successful): don't add LOC,
		//         just add CEN pushing to 2) (partial CEN data list).
		//    1.2. It isn't (there is no entry or comparisons are unsuccessful): add LOC,
		//         add new LOC to 1), add entry to 2), copy previous file data to the output
		//         file, and increment output file size by LOC size + file data size.

		let previous_file =
			if let Some(previous_file) = self.previous_zip_contents.get(file_path.as_ref()) {
				previous_file
			} else {
				return Err(SquashZipError::NoSuchPreviousFile(String::from(
					file_path.as_ref()
				)));
			};

		let previous_zip = self.previous_zip.as_mut().unwrap();

		let mut local_file_header = LocalFileHeader::new(file_path.as_ref())?;
		local_file_header.squash_time = SYSTEM_TIME_SANITIZER.sanitize(
			&previous_file.squash_time,
			&previous_file.crc32.to_le_bytes() // TODO: obfuscation
		)?;
		local_file_header.crc32 = previous_file.crc32; // TODO: obfuscation
		local_file_header.compression_method = previous_file.compression_method; // TODO: obfuscation
		local_file_header.uncompressed_size = previous_file.uncompressed_size; // TODO: obfuscation
		local_file_header.compressed_size = previous_file.compressed_size; // TODO: obfuscation

		let mut empty_vec;
		let matching_local_headers = if self.settings.strict_spec_conformance {
			// We can't reuse local file headers if strict conformance is desired.
			// Consider that no headers ever match
			empty_vec = Vec::new();
			&mut empty_vec
		} else {
			self.processed_local_headers
				.entry(HashAndSize {
					hash: previous_file.crc32,           // TODO: obfuscation
					size: previous_file.compressed_size  // TODO: obfuscation
				})
				.or_insert_with(|| Vec::with_capacity(1)) // Usually, this list will be small
		};

		let get_partial_central_directory_header =
			|local_header_offset: u64| -> Result<PartialCentralDirectoryHeader, SquashZipError> {
				Ok(PartialCentralDirectoryHeader {
					local_header_offset,
					file_name: String::from(file_path.as_ref()), // TODO: obfuscation
					compression_method: previous_file.compression_method, // TODO: obfuscation
					squash_time: SYSTEM_TIME_SANITIZER.sanitize(
						&previous_file.squash_time,
						&previous_file.crc32.to_le_bytes()
					)?, // TODO: obfuscation
					crc32: previous_file.crc32,                  // TODO: obfuscation
					compressed_size: previous_file.compressed_size, // TODO: obfuscation
					uncompressed_size: previous_file.uncompressed_size  // TODO: obfuscation
				})
			};

		let local_file_header_offset = self.output_zip.seek(SeekFrom::Current(0)).await?;
		let mut already_stored = false;
		for (matching_header_offset, matching_header_size) in &*matching_local_headers {
			let matching_data_start_offset = matching_header_offset + *matching_header_size as u64;

			// Read from the output ZIP file the matching data
			self.output_zip
				.seek(SeekFrom::Start(matching_data_start_offset))
				.await?;

			let matching_output_zip_data =
				ReaderStream::new((&mut self.output_zip).take(previous_file.compressed_size as u64))
					.map_ok(|byte_chunk| {
						tokio_stream::iter(byte_chunk).map(Result::<u8, io::Error>::Ok)
					})
					.try_flatten();

			// Now do the same with the previous ZIP file data
			previous_zip
				.seek(SeekFrom::Start(previous_file.data_offset))
				.await?;

			let previous_zip_data =
				ReaderStream::new(previous_zip.take(previous_file.compressed_size as u64))
					.map_ok(|byte_chunk| tokio_stream::iter(byte_chunk).map(Result::Ok))
					.try_flatten();

			// Use stream magic to check whether the bytes in both regions are the same.
			// FIXME: this doesn't fail fast in case an error or inequality is found
			already_stored = match matching_output_zip_data
				.zip(previous_zip_data)
				.fold(
					(true, None),
					|(equal_bytes, err), (output_byte, previous_byte)| async move {
						match output_byte {
							Ok(output_byte) => match previous_byte {
								Ok(previous_byte) => {
									(equal_bytes && output_byte == previous_byte, err)
								}
								Err(err) => (equal_bytes, Some(err))
							},
							Err(err) => (equal_bytes, Some(err))
						}
					}
				)
				.await
			{
				(equal_bytes, None) => equal_bytes,
				(_, Some(err)) => return Err(err.into())
			};

			if already_stored {
				self.central_directory_data
					.push(get_partial_central_directory_header(
						*matching_header_offset
					)?);

				// Make sure the next local file header is where it should be
				// for the following operations
				self.output_zip
					.seek(SeekFrom::Start(local_file_header_offset))
					.await?;

				// Found a match. No point in finding more
				break;
			}
		}

		if !already_stored {
			let local_file_header_size = local_file_header.get_size();

			// If some unsucessful comparison was done, the stream position of the output
			// ZIP file will not be that of the to be added local file header
			if !matching_local_headers.is_empty() {
				self.output_zip
					.seek(SeekFrom::Start(local_file_header_offset))
					.await?;
			}

			if !self.settings.strict_spec_conformance {
				// Avoid allocating memory for the dummy vector
				matching_local_headers.push((local_file_header_offset, local_file_header_size));
			}

			self.central_directory_data
				.push(get_partial_central_directory_header(
					local_file_header_offset
				)?);

			self.output_zip_size +=
				local_file_header_size as u64 + previous_file.compressed_size as u64; // TODO: obfuscation

			local_file_header.write(&mut self.output_zip).await?;

			previous_zip
				.seek(SeekFrom::Start(previous_file.data_offset))
				.await?;

			tokio::io::copy(
				&mut previous_zip.take(previous_file.compressed_size as u64), // TODO: obfuscation
				&mut self.output_zip
			)
			.await?;
		}

		Ok(())
	}

	/// Finishes this ZIP file, writing any needed remaining data structures and flushing all
	/// the data to a new file in the specified path.
	///
	/// This operation ends the lifecycle of this SquashZip instance, consuming it, so no
	/// further operations can be done on the ZIP file after this method returns.
	async fn finish<P: AsRef<Path>>(mut self, path: P) -> Result<(), SquashZipError> {
		let central_directory_entry_count = u64::try_from(self.central_directory_data.len())?;

		// First, write the central directory file headers. Remember where they are
		let central_directory_start_offset = self.output_zip.seek(SeekFrom::Current(0)).await?;
		for header_data in self.central_directory_data {
			// TODO: obfuscation
			let central_directory_header = CentralDirectoryHeader::new(
				&header_data.file_name,
				header_data.local_header_offset,
				header_data.compression_method,
				header_data.squash_time,
				header_data.crc32,
				header_data.compressed_size,
				header_data.uncompressed_size,
				0,     // Local header disk
				false  // Spoof version made by
			);

			central_directory_header.write(&mut self.output_zip).await?;
			self.output_zip_size += central_directory_header.get_size() as u64;
		}
		let central_directory_end_offset = self.output_zip.seek(SeekFrom::Current(0)).await?;

		// Now write the end of central directory
		// TODO: obfuscation
		let end_of_central_directory = EndOfCentralDirectory::new(
			0,                                                             // Number of this disk
			0,                                                             // Central directory start disk number
			central_directory_entry_count,                                 // Entries in CD in this disk
			central_directory_entry_count,                                 // Total entries in CD (all disks)
			central_directory_end_offset - central_directory_start_offset, // CD size
			central_directory_start_offset,                                // CD start offset
			0,                                                             // Number of disks
			central_directory_end_offset,                                  // Current file offset
			0,     // ZIP64 record size offset (for obfuscation; zero means write the real value)
			false, // Spoof version made by
			false  // Zero out unused ZIP64 fields
		);

		end_of_central_directory.write(&mut self.output_zip).await?;
		self.output_zip_size += end_of_central_directory.get_size() as u64;

		// Finally, write the generated ZIP file to its place!
		// This also implicitly flushes any buffer, so any error during flushing will be returned
		self.output_zip.seek(SeekFrom::Start(0)).await?;
		tokio::io::copy(
			&mut AsyncReadExt::take(self.output_zip, self.output_zip_size),
			&mut File::create(path).await?
		)
		.await
		.map_or_else(|err| Err(err.into()), |_| Ok(()))
	}
}
